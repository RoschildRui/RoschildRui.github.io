---
layout:       post
title:        "[LLM Prompt Recovery](PV:0.6694)é“¶ç‰Œæ–¹æ¡ˆæ€»ç»“(1)"
author:       "Roschild.Rui"
header-style: text
catalog:      true

---
> â€œè¿™æ˜¯æˆ‘çš„ç¬¬ä¸€ä¸ªblogï¼Œå¸Œæœ›åœ¨è¿™ä¸€è·¯ä¸Šèƒ½è®¤è¯†æ›´å¤šå¿—åŒé“åˆçš„æœ‹å‹â€

> "This is my first blog post. Setting out on this path, I am looking forward to meeting more friends who also have greate passion on data science. Together, we will dive into the fascinating world of data, uncover valuable and deeper insights on this Earth, and strive to make a meaningful impact to make people life great 'again' !!!"

### å†™åœ¨å‰é¢
åœ¨è¿™ä¸€ç¯‡blogä¸­æˆ‘ä¼šå…ˆç®€å•ä»‹ç»æˆ‘ä»¬å›¢é˜Ÿçš„æœ€ç»ˆæäº¤æ–¹æ¡ˆ**(`PV=0.6694`,`PB=0.6659`)**

éšåæ•´ä¸ªæ–¹æ¡ˆçš„è¿­ä»£å½¢æˆè¿‡ç¨‹å°†åœ¨æˆ‘æ¥ä¸‹æ¥çš„blogä¸­é€ä¸€å‘ˆç°

åŒæ—¶ï¼Œæˆ‘ä»¬ä¹Ÿä¼šåœ¨æ¥ä¸‹æ¥çš„blogä¸­é€šè¿‡å¤ç°é‡‘ç‰ŒåŒºä»¥åŠéƒ¨åˆ†ä¼˜ç§€é“¶ç‰ŒåŒºçš„æ¯”èµ›æ–¹æ¡ˆï¼Œåæ€æ€»ç»“ï¼Œä»¥æœŸåŠ æ·±æˆ‘ä»¬å¯¹äºå¤§æ¨¡å‹çš„ç†è§£

æ„Ÿè°¢ [Kaggle](https://www.kaggle.com/) â€”â€”ä¸€ä¸ªå¼€æ”¾ï¼Œå¯Œæœ‰æ´»åŠ›çš„æ•°æ®ç§‘å­¦ç¤¾åŒºç«èµ›å¹³å°ï¼Œä¸ºæˆ‘ä»¬æä¾›äº†ä¸€æ¬¡æ·±å…¥ç†è§£LLMå†…éƒ¨æœºåˆ¶çš„æœºä¼šã€‚ğŸ˜‰

### [æ¯”èµ›å†…å®¹](https://www.kaggle.com/competitions/llm-prompt-recovery)
**Overview**

LLMs are commonly used to rewrite or make stylistic changes to text. The goal of this competition is to recover the LLM prompt that was used to transform a given text.

**Description**

NLP workflows increasingly involve rewriting text, but there's still a lot to learn about how to **prompt LLMs effectively**. This machine learning competition is designed to be a novel way to dig deeper into this problem.

The challenge: recover the LLM prompt used to rewrite a given text. Youâ€™ll be tested against a dataset of 1300+ original texts, each paired with a rewritten version from Gemma, Googleâ€™s new family of open models.

**Evaluation**

Evaluation Metric

For each row in the submission and corresponding ground truth, sentence-t5-base is used to calculate corresponding embedding vectors. The score for each predicted / expected pair is calculated using the Sharpened Cosine Similarity, using an exponent of 3. The SCS is used to attenuate the generous score given by embedding vectors for incorrect answers. Do not leave any rewrite_prompt blank as null answers will throw an error.

Submission File

The submission file should contain a header and have the following format:

```csv
id,rewrite_prompt
000aaa,"Rewrite this essay but do it using the writing style of Dr. Seuss"
111bbb,"Rewrite this essay but do it using the writing style of William Shakespeare"
222ccc,"Rewrite this essay but do it using the writing style of Tupac Shakur"
...
```


### æ–¹æ¡ˆæ€»ç»“
- 1.æ„å»ºåŸºäºdeberta-v3-largeçš„seq2seqæ¨¡å‹
- 2.æ„å»ºåˆé€‚çš„adapterå±‚å¾®è°ƒ[phi2](https://www.kaggle.com/models/Microsoft/phi/Transformers/2/1)æ¨¡å‹
- 3.few-shot [mistral-7b-v2](https://www.kaggle.com/datasets/ahmadsaladin/mistral-7b-it-v02)æ¨¡å‹

å°†ä¸‰ä¸ªæ¨¡å‹è¿˜åŸçš„æç¤ºè¯é›†æˆåœ¨ä¸€èµ·ä½œä¸ºæœ€ç»ˆçš„é¢„æµ‹ç»“æœ

### è¯„ä»·æŒ‡æ ‡çš„ç†è§£

æˆ‘å°†é€šè¿‡ä¸€ä¸ªå…·ä½“çš„æ•°å­¦ä¾‹å­æ¥å±•ç¤ºé”åŒ–ä½™å¼¦ç›¸ä¼¼åº¦ä¸ä¼ ç»Ÿä½™å¼¦ç›¸ä¼¼åº¦ä¹‹é—´çš„åŒºåˆ«ã€‚è¿›è€Œæ›´å¥½åœ°ç†è§£æ¯”èµ›è¯„ä»·æŒ‡æ ‡çš„å€¾å‘æ€§ã€‚

**ç¤ºä¾‹å‘é‡**

å‡è®¾æˆ‘ä»¬æœ‰ä¸¤å¯¹å‘é‡ï¼Œä¸€å¯¹è¾ƒä¸ºç›¸ä¼¼ï¼Œå¦ä¸€å¯¹è¾ƒä¸ºä¸ç›¸ä¼¼ï¼š

- å‘é‡å¯¹Aï¼ˆç›¸ä¼¼ï¼‰:

```math
  $\vec{u}$ = [1, 2, 3.0]
  $\vec{v}$ = [1, 2, 2.9]
```

- å‘é‡å¯¹Bï¼ˆä¸ç›¸ä¼¼ï¼‰:

```math
  $\vec{x}$ = [1, 2, 3]
  $\vec{y}$ = [3, 2, 1] 
```

**è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦**

ä½™å¼¦ç›¸ä¼¼åº¦å…¬å¼ä¸ºï¼š

$\text{cosine similarity}$ = $\frac{\vec{a} \cdot \vec{b}}{\|\vec{a}\| \|\vec{b}\|}$

å…¶ä¸­ $\vec{a} \cdot \vec{b} $æ˜¯å‘é‡çš„ç‚¹ç§¯ï¼Œ$\|\vec{a}\|$ å’Œ $\|\vec{b}\|$ æ˜¯å‘é‡çš„æ¨¡ã€‚
 
å¯¹äºå‘é‡å¯¹Aå’ŒBï¼Œè®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦ï¼š

- å¯¹A:

```math
  $ \vec{u} \cdot \vec{v} = 1*1 + 2*2 + 3*2.9 = 1 + 4 + 8.7 = 13.7 $
  $ \|\vec{u}\| = \sqrt{1^2 + 2^2 + 3^2} = \sqrt{14} $
  $ \|\vec{v}\| = \sqrt{1^2 + 2^2 + 2.9^2} \approx \sqrt{13.61} $
  $ \text{cosine similarity}_{A} = \frac{13.7}{\sqrt{14} \times \sqrt{13.61}} \approx 0.994 $
```

- å¯¹B:

```math
  $ \vec{x} \cdot \vec{y} = 1*3 + 2*2 + 3*1 = 3 + 4 + 3 = 10 $
  $ \|\vec{x}\| = \sqrt{14}, \|\vec{y}\| = \sqrt{14} $
  $ \text{cosine similarity}_{B} = \frac{10}{14} \approx 0.535 $
```

**åº”ç”¨é”åŒ–å¤„ç†ï¼ˆ`p = 3`ï¼‰**

é”åŒ–ä½™å¼¦ç›¸ä¼¼åº¦ä¸º $`\text{cosine similarity}^p`$ï¼Œè¿™é‡Œå– `p = 3`ï¼š

- å¯¹A:

```math
  $ \text{sharpened cosine similarity}_{A} = 0.994^3 \approx 0.982 $
```

- å¯¹B:

```math
  $ \text{sharpened cosine similarity}_{B} = 0.535^3 \approx 0.153 $
```

**åˆ†æç»“æœ**

åœ¨ä¸Šè¿°ç¤ºä¾‹ä¸­ï¼Œå‘é‡å¯¹Açš„ä½™å¼¦ç›¸ä¼¼åº¦å¾ˆé«˜ï¼ˆæ¥è¿‘1ï¼‰ï¼Œé€šè¿‡é”åŒ–å¤„ç†ï¼Œå®ƒçš„ç›¸ä¼¼åº¦å€¼è™½æœªè¢«è¿›ä¸€æ­¥å¢å¼ºï¼Œä½†å‡å°‘ç›¸å¯¹æœ‰é™ã€‚å¯¹äºå‘é‡å¯¹Bï¼Œé”åŒ–å¤„ç†æ˜¾è‘—é™ä½äº†å®ƒçš„ç›¸ä¼¼åº¦å€¼ï¼Œå‘é‡å¯¹Aä¸å‘é‡å¯¹Bçš„å·®å€¼æ˜¾è‘—æé«˜äº†ã€‚

é€šè¿‡é”åŒ–ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œæˆ‘ä»¬å¯ä»¥æ˜æ˜¾åœ°åŒºåˆ†é«˜åº¦ç›¸ä¼¼ã€ä¸€èˆ¬ç›¸ä¼¼ã€å’Œè¿¥å¼‚çš„æƒ…å†µï¼Œé”åŒ–ä½™å¼¦ç›¸ä¼¼åº¦é€šè¿‡**å¼ºåŒ–å·²æœ‰çš„ç›¸ä¼¼åº¦æˆ–å·®å¼‚**ï¼Œ**ä½¿å¾—æ¨¡å‹é¢„æµ‹å€¼æ›´åŠ â€œå°–é”â€**ã€‚

è¿™è¯´æ˜è¿™åœºæ¯”èµ›ä¼šä»¥è¾ƒé«˜çš„ç²¾åº¦åŒºåˆ†éå¸¸ç›¸è¿‘ã€ä¸€èˆ¬ç›¸è¿‘ã€æˆ–è€…è¿¥å¼‚çš„å®ä½“æ–‡æœ¬ã€‚è¿›è€Œå¯¹äºæˆ‘ä»¬çš„åµŒå…¥å‘é‡é¢„æµ‹çš„å‡†ç¡®æ€§æå‡ºäº†å¾ˆé«˜çš„è¦æ±‚ã€‚**ï¼ˆä¹Ÿä¸ºæˆ‘ä»¬åé¢é›†æˆå¤šä¸ªæ¨¡å‹è¿›è¡Œé¢„æµ‹è¾“å‡ºåŸ‹ä¸‹äº†ä¼ç¬”ï¼‰** ğŸ˜

### æ•°æ®é›†çš„æ„å»º

å› ä¸ºæ¯”èµ›æ–¹æä¾›çš„æ•°æ®**æœ‰é™ç”šè‡³ç­‰åŒäºæ²¡æœ‰**ï¼Œè®­ç»ƒé›†å’Œæµ‹è¯•é›†éƒ½åªæœ‰**ä¸€æ¡**ğŸ˜°ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦é¢å¤–æŸ¥æ‰¾ç”Ÿæˆä¸€äº›æ•°æ®ä»¥æ–¹ä¾¿è®­ç»ƒ**ç«¯åˆ°ç«¯æ¨¡å‹**ä»¥åŠ**æ„å»ºæœ¬åœ°cvåº“**

#### æ•°æ®æ¥æº
ä»¥ä¸‹æ˜¯æˆ‘ä»¬çš„æ•°æ®æ¥æºï¼Œè¿™é‡Œå¿…é¡»è¦æ„Ÿè°¢kaggle**å¼€æºæ•°æ®é›†çš„å¤§ä½¬ä»¬**ï¼Œä»¥åŠ**æ•´ç†å¼€æºæ•°æ®çš„å¼€æºå¤§ä½¬ä»¬**ï¼ï¼ï¼ğŸ¥³ğŸ¥³ğŸ¥³

**reference**
- [https://www.kaggle.com/code/tomooinubushi/all-in-one-dataset-with-embedding/notebook](https://huggingface.co/datasets/Skylion007/openwebtext)
- [https://huggingface.co/datasets/Skylion007/openwebtext](https://huggingface.co/datasets/Skylion007/openwebtext)
- [https://huggingface.co/datasets/euclaise/writingprompts/viewer/default/train?p=1](https://huggingface.co/datasets/euclaise/writingprompts/viewer/default/train?p=1)
- [https://www.kaggle.com/datasets/nbroad/gemma-rewrite-nbroad](https://www.kaggle.com/datasets/nbroad/gemma-rewrite-nbroad)
- [https://www.kaggle.com/datasets/ilanmeissonnier/chatgpt-rewrite-promts/data](https://www.kaggle.com/datasets/ilanmeissonnier/chatgpt-rewrite-promts/data)
- [https://www.kaggle.com/datasets/winddude/70k-prompt-rewrite-triples/data](https://www.kaggle.com/datasets/winddude/70k-prompt-rewrite-triples/data)
- [https://www.kaggle.com/code/aatiffraz/generating-data-through-gemma7b-1000-texts-5h](https://www.kaggle.com/code/aatiffraz/generating-data-through-gemma7b-1000-texts-5h)
- ...

#### æ•°æ®é¢„å¤„ç†
æˆ‘ä»¬å‘ç°æ•°æ®é›†ä¸­å­˜åœ¨ä¸€äº›æ— æ•ˆä¿¡æ¯ï¼Œä»¥åŠä¸€äº›æœ‰æ•ˆä¿¡æ¯å­˜åœ¨å™ªå£°ï¼Œäºæ˜¯æˆ‘ä»¬ä¸»è¦å‚è€ƒä¸€ä½kaggleçš„experté¢„å¤„ç†æ–¹æ¡ˆå¯¹æ•°æ®é›†è¿›è¡Œäº†**æ­£åˆ™å»å™ª**ï¼ˆæŠ±æ­‰ï¼Œæ‰¾ä¸åˆ°é‚£ä¸ªæ–¹æ¡ˆçš„è¯´æ˜å»å“ªé‡Œäº†ï¼Œæ‰€ä»¥å°±è®©Claudeå†™ä¸€ä¸‹å§ğŸ˜‹ï¼‰

å»å™ªå†…å®¹å¦‚ä¸‹ï¼š
1. å»é™¤æ— ç”¨æˆ–ä¸ç›¸å…³çš„æ–‡æœ¬:
   - åœ¨ç”Ÿæˆçš„`rewritten_text`ä¸­,åŒ…å«ä¸€äº›æ— ç”¨æˆ–ä¸ç›¸å…³çš„æ–‡æœ¬,å¦‚"therefore.*I cannot"ã€"does not contain any"ç­‰ã€‚
   - è¿™äº›æ–‡æœ¬æ˜¯ç”±è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„,ä½†å¹¶ä¸æ˜¯æˆ‘ä»¬æƒ³è¦çš„é‡å†™åçš„æ–‡æœ¬ã€‚
   - é€šè¿‡ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…è¿™äº›æ¨¡å¼,å¹¶å°†åŒ¹é…åˆ°çš„æ–‡æœ¬æ¸…ç©º,å¯ä»¥å»é™¤è¿™äº›æ— ç”¨æˆ–ä¸ç›¸å…³çš„æ–‡æœ¬ã€‚

2. æå–ç›¸å…³çš„æ–‡æœ¬ç‰‡æ®µ:
   - åœ¨æŸäº›æƒ…å†µä¸‹,`rewritten_text`ä¸­åŒ…å«ä¸€äº›å›ºå®šçš„æ¨¡å¼,å¦‚"Sure, here.*?:"ã€"Summary of .*?\n\n"ç­‰ã€‚
   - è¿™äº›æ¨¡å¼åé¢é€šå¸¸è·Ÿç€æˆ‘ä»¬çœŸæ­£æ„Ÿå…´è¶£çš„é‡å†™åçš„æ–‡æœ¬ã€‚
   - é€šè¿‡ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…è¿™äº›å­æ¨¡å¼,å¹¶ä»åŒ¹é…ä½ç½®çš„æœ«å°¾å¼€å§‹æå–æ–‡æœ¬,å¯ä»¥è·å–åˆ°ç›¸å…³çš„æ–‡æœ¬ç‰‡æ®µã€‚

3. å»é™¤å¤šä½™çš„å‰ç¼€æˆ–åç¼€:
   - æœ‰æ—¶ç”Ÿæˆçš„`rewritten_text`åŒ…å«ä¸€äº›å¤šä½™çš„å‰ç¼€æˆ–åç¼€,å¦‚ä»¥"**\n\n"æˆ–"user"å¼€å¤´çš„æ–‡æœ¬ã€‚
   - è¿™äº›å‰ç¼€æˆ–åç¼€æ˜¯ç”±ä¸åŒè¯­è¨€æ¨¡å‹çš„ä¸€äº›ç‰¹æ€§ç”Ÿæˆçš„,ä½†å¯¹äºé‡å†™åçš„æ–‡æœ¬æ¥è¯´æ˜¯å¤šä½™çš„ã€‚
   - é€šè¿‡å»æ‰è¿™äº›å¤šä½™çš„å‰ç¼€æˆ–åç¼€,è·å¾—æ›´å¹²å‡€å’Œå‡†ç¡®çš„é‡å†™åçš„æ–‡æœ¬ã€‚

4. å¤„ç†ç‰¹æ®Šæƒ…å†µ:
   - `fix_prompt`å‡½æ•°ä¸­çš„æ­£åˆ™è¡¨è¾¾å¼å¯ä»¥å¤„ç†ä¸€äº›ç‰¹æ®Šæƒ…å†µã€‚
   - ä¾‹å¦‚,åŒ¹é…"therefore.*I cannot"æ¨¡å¼çš„æ–‡æœ¬è¡¨ç¤ºè¿™ä¸ªå¤§è¯­è¨€æ¨¡å‹å¤§æ¦‚ç‡æ— æ³•ç”Ÿæˆåˆé€‚çš„é‡å†™,å› æ­¤å°†å…¶æ¸…ç©ºã€‚
   - åŒ¹é…"Sure, here.*?:"æ¨¡å¼çš„æ–‡æœ¬å¯èƒ½è¡¨ç¤ºè¯­è¨€æ¨¡å‹ç”Ÿæˆäº†ä¸€äº›å›ºå®šçš„å›å¤æ ¼å¼,éœ€è¦æå–å…¶åçš„ç›¸å…³æ–‡æœ¬ã€‚

**å‚è€ƒä»£ç å¦‚ä¸‹ï¼š**
```python
import re

def fix_prompt(text):
    patterns = [
        r'therefore.*I cannot',
        "does not contain any",
        'am unable to provide',
        "am unable to rewrite",
        "do not have the capacity to write",
        "am unable to engage",
        "I am unable to",
        "not provide information",
    ]
    sub_patterns= [
        r"Sure, here.*?:",
        r"Sure. here.*?:",
        r"Certainly, here.*?:",
        r"here.*? text*?:",
        r"Summary of .*?\n\n",
        r"Analysis of .*?\n\n",
        #r"The text.*?:"
    ]
    for p in patterns:
        match = re.search(p, text, re.IGNORECASE)
        
        if match:
            text = ''
    
    if text=="":
        return text
    else:
        for p in sub_patterns:
            match = re.search(p, text, re.IGNORECASE)
            if match:
                text = text[match.end():]
        if text.startswith('**\n\n') or text.startswith('user'):
            text=text[4:].strip()
        return text
```

#### embeddingæ•°æ®
å°†ä¸Šè¿°æ­¥éª¤å¤„ç†å¥½çš„æ•°æ®ï¼Œé€šè¿‡**sentence-t5-base**æ¨¡å‹ï¼Œç”Ÿæˆè®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„embedding

åŒæ—¶å°†ä¸Šè¿°æ•°æ®ä¸­çš„unique promptæç¤ºè¯æ•´ç†ä¸ºä¸€ä¸ªpromptæ–‡ä»¶ï¼Œåˆ©ç”¨Metaå¼€æºçš„Faissåº“å°†å…¶è½¬ä¸ºprompt.indexï¼Œå¯¹debertaæ¨¡å‹é¢„æµ‹ç»“æœè¿›è¡Œç›¸ä¼¼åº¦åŒ¹é…è¿›è€Œè¾“å‡º**ï¼ˆå…³äºä¸ºä»€ä¹ˆç”¨è¿™ç§æ–¹æ³•è¿™é‡Œåªè®²ä¸€ç‚¹å‰©ä¸‹çš„åé¢ä¼šè¯¦ç»†è®²è§£---è¿™ç§æ–¹æ³•æ¶¨ç‚¹æ˜¾è‘—ä½†æ˜¯è¦æ±‚ç§æœ‰æ•°æ®é›†æ„å»ºå®Œå–„ï¼Œæˆ‘ä»¬é€šè¿‡ä½¿ç”¨æç¤ºè¯å·¥ç¨‹è°ƒç”¨gpt-4ç”Ÿæˆäº†150æ¡é«˜è´¨é‡å¹³å‡æç¤ºè¯ï¼ˆ`PB=0.58`ä»¥ä¸Šï¼ŒåŒæ—¶åœ¨å¼€æºçš„æ•°æ®é›†ä¸­æ‰¾åˆ°äº†1400000ä½™æ¡å¯Œæœ‰ç‰¹å¾çš„æç¤ºè¯,æ²¡é”™å°±æ˜¯140wæ¡ä½ æ²¡çœ‹é”™ğŸ˜€ï¼‰**

**å‚è€ƒä»£ç å¦‚ä¸‹ï¼š**
````python
import pandas as pd
import gc
import numpy as np
import pandas as pd
import time
from tqdm import tqdm
import numpy as np
from sentence_transformers import SentenceTransformer
import pickle

df = pd.read_parquet(f"./train_clean.parquet", columns=['rewrite_prompt'])
test= pd.read_csv('./test.csv', usecols=['rewrite_prompt'])

model =  SentenceTransformer('sentence-transformers/sentence-t5-base')
model.max_seq_length = 512

encoded_data = model.encode(list(df['rewrite_prompt']), batch_size=32, device='cuda', show_progress_bar=True, convert_to_tensor=True, normalize_embeddings=True)
encoded_data = encoded_data.detach().cpu().numpy()
encoded_data = np.asarray(encoded_data.astype('float32'))

np.save('train_emb_sentence-t5.npy', encoded_data)

test_emb = model.encode(list(test['rewrite_prompt']), batch_size=32, device='cuda', show_progress_bar=True, convert_to_tensor=True, normalize_embeddings=True)
test_emb = test_emb.detach().cpu().numpy()
test_emb = np.asarray(test_emb.astype('float32'))

np.save('test_emb_sentence-t5.npy', test_emb)
```

```python
import sys
from sentence_transformers import SentenceTransformer, models
import pandas as pd
import gc
import numpy as np
import faiss
import time
from tqdm import tqdm

df = pd.read_csv(f"prompts_df.csv",)
contexts = list(df['rewrite_prompt'])

model =  SentenceTransformer('sentence-transformers/sentence-t5-base')
model.max_seq_length = 512

encoded_data = model.encode(contexts, batch_size=32, device='cuda', show_progress_bar=True, convert_to_tensor=True, normalize_embeddings=True)
encoded_data = encoded_data.detach().cpu().numpy()
encoded_data = np.asarray(encoded_data.astype('float32'))
df['rewrite_prompt'].to_csv('prompts_df.csv', index=False)

index = faiss.IndexFlatIP(768)
index.add(encoded_data)
faiss.write_index(index, 'prompts_embedding.index')
```

### è®­ç»ƒdebertaæ¨¡å‹
**[Reference](https://www.kaggle.com/code/alejopaullier/llm-pr-seq2seq-train/notebook)**
æˆ‘ä»¬ä¸»è¦åŸºäºè¿™ä¸ªç¬”è®°æœ¬è¿›è¡Œäº†ä¸€äº›æå‡ä¿®æ”¹
#### æ¨¡å‹å‚æ•°è®¾ç½®
```python
class config:
    AMP = True
    BATCH_SIZE_TRAIN = 4
    BATCH_SIZE_VALID = 4
    BETAS = (0.9, 0.999)
    DEBUG = 0 
    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    LR = 5e-6
    EPOCHS = 10
    EPS = 1e-8
    GRADIENT_CHECKPOINTING = False
    MODEL = "/kaggle/input/deberta-v3-large-hf-weights"
    CKPT = 'deberta-v3-large'
    MAX_GRAD_NORM = 250000.0
    MAX_LEN = 384
    NUM_WORKERS = 0
    PRINT_FREQ = 500
    SEED = 42
    WANDB = False
    WEIGHT_DECAY = 0.005
```

#### æ¨¡å‹ç»“æ„
æ¨¡å‹ç»“æ„ä¸­æœ‰ä¸¤ä¸ªtricks
- 1.ç›´æ¥ä½¿ç”¨debertaçš„é¢„è®­ç»ƒæƒé‡æå–åŸå§‹æ–‡æœ¬å’Œé‡å†™æ–‡æœ¬çš„ç‰¹å¾ï¼Œæˆ‘ä»¬å‘ç°å…¨å‚æ•°è®­ç»ƒå’Œä½¿ç”¨é¢„è®­ç»ƒæƒé‡åœ¨æµ‹è¯•é›†ä¸Šçš„è¡¨ç°**æ²¡æœ‰æ˜¾è‘—çš„å·®åˆ«**ï¼Œåœ¨äº¤å‰éªŒè¯çš„æ—¶å€™ç”šè‡³å‘ç°åœ¨æŸäº›æ—¶å€™ä¼š**å¼±äº**é¢„è®­ç»ƒæƒé‡ï¼Œäºæ˜¯æˆ‘ä»¬å†³å®šç›´æ¥ä½¿ç”¨**é¢„è®­ç»ƒçš„æƒé‡**å¹¶è®¾è®¡äº†ä¸€ä¸ª**å¤´ç»“æ„**ï¼Œè®©è¿™ä¸ªå¤´èƒ½ä»åº•å±‚æ¨¡å‹debertaä¸­æå–çš„ä¸°å¯Œç‰¹å¾ä¸­å­¦ä¹ åˆ°æœ‰ç”¨çš„è¡¨ç¤ºï¼Œè¿›è€Œé€šè¿‡å˜æ¢å’Œå‹ç¼©ï¼Œç”Ÿæˆèƒ½å¤Ÿæœ‰æ•ˆé¢„æµ‹é‡å†™æç¤ºçš„åµŒå…¥å‘é‡ã€‚å…³äºä¸ºä»€ä¹ˆå°†ä¸­é—´å±‚ç»´åº¦è®¾ä¸º32256ï¼Œç®€å•æ¥è¯´å°±æ˜¯ç„å­¦ğŸ˜…ï¼Œç¡¬è¦è¯´å°±æ˜¯768*42ï¼Œä¸€èˆ¬å°†**ä¸­é—´å±‚å‘é‡ç»´åº¦è®¾ä¸ºåµŒå…¥å‘é‡ç»´åº¦çš„nå€**ä¼šå–å¾—ä¸é”™çš„æ•ˆæœğŸ˜Šï¼Œè¿™é‡Œæˆ‘ä»¬ç›´æ¥ä»  `n=36`å¼€å§‹å°è¯•æœ€ç»ˆå‘ç°`n=42`å–å¾—äº†ä¸é”™çš„æ•ˆæœã€‚ï¼ˆæˆ‘çš„è¯„ä»·æ˜¯ç»éªŒï¼Œå› ä¸ºæˆ‘ä»¬æ—¢å¸Œæœ›æ¨¡å‹èƒ½ä»debertaæå–åˆ°çš„ç‰¹å¾ä¸­å­¦åˆ°**æ›´ä¸°å¯Œçš„è¯­ä¹‰ä¿¡æ¯**åˆå¸Œæœ›ä¸è¦**overfitting**ï¼Œå¦‚æœè§‰å¾—å¤ªç„å­¦ï¼Œç›´æ¥ä½¿ç”¨åé¢ä¸¤ä¸ªæ¨¡å‹é›†æˆæ•ˆæœä¹Ÿè¶³å¤Ÿå–å¾—ä¸é”™çš„æ•ˆæœ**(`PV=0.6573`,`PB=0.6569`åœ¨LBä¸­ç§æ¦œæ’81åï¼ŒåŒæ ·æ˜¯é“¶ç‰Œä½)**ï¼Œè¿™ä¸ªseq2seqæ¨¡å‹å°±å½“çœ‹ä¸€ä¸ªä¹å­äº†ğŸ˜‡ï¼‰
- 2.åœ¨è®¾è®¡çš„å¤´ç»“æ„ä¸­ä½¿ç”¨BatchNormä»£æ›¿LayNormï¼ˆåœ¨å¤šæ¬¡äº¤å‰éªŒè¯ä¸­å¹³å‡æ¶¨ç‚¹**0.003**ï¼‰ï¼Œè¿™æˆ‘è§‰å¾—åªèƒ½ç®—æ˜¯ï¼Œ**å››ä¸ªç‰¹å®š**ï¼Œç‰¹å®šä»»åŠ¡ã€ç‰¹å®šåµŒå…¥æ¨¡å‹ã€ç‰¹å®šè¯„ä»·æŒ‡æ ‡ã€ç‰¹å®šæ•°æ®é›†çš„trick ï¼ˆæˆ‘å°†batch_sizeè®¾ä¸º2ä¾æ—§å¦‚æ­¤ï¼‰ ğŸ¤”

å‚è€ƒä»£ç å¦‚ä¸‹ï¼š
```python
class CustomModel(nn.Module):
    def __init__(self, cfg, config_path=None, mode: str ="train", pretrained=False): 
        super().__init__()
        self.cfg = cfg
        self.mode = mode
        self.dropout = 0.2
        if config_path is None:
            self.config = AutoConfig.from_pretrained(cfg.MODEL, output_hidden_states=True) # output_hidden_states=True è¡¨ç¤ºåœ¨é…ç½®ä¸­å¯ç”¨è¾“å‡ºéšè—çŠ¶æ€çš„é€‰é¡¹,å³åœ¨å‰å‘ä¼ æ’­è¿‡ç¨‹ä¸­ä¿å­˜æ¯ä¸€å±‚çš„éšè—çŠ¶æ€ã€‚
            self.config.hidden_dropout = 0.
            self.config.hidden_dropout_prob = 0. # Dropoutç‡
            self.config.attention_dropout = 0. # æ³¨æ„åŠ›æƒé‡çš„dropoutç‡
            self.config.attention_probs_dropout_prob = 0.

        else:
            self.config = torch.load(config_path)

        if pretrained:
            self.model = AutoModel.from_pretrained(cfg.MODEL, config=self.config) #self.configï¼šè¿™æ˜¯ä¸€ä¸ªé…ç½®å¯¹è±¡ï¼ŒåŒ…å«äº†æ¨¡å‹åº”æœ‰çš„é…ç½®ã€‚ç”¨æ¥ä¿®æ”¹é»˜è®¤çš„æ¨¡å‹é…ç½®ï¼Œå¦‚è°ƒæ•´dropoutç‡æˆ–å…¶ä»–æ¶æ„ç›¸å…³çš„è®¾ç½®ã€‚
        else:
            self.model = AutoModel(self.config)

        """
        åŠŸèƒ½ï¼šç”¨äºå¯ç”¨æ¨¡å‹çš„æ¢¯åº¦æ£€æŸ¥ç‚¹åŠŸèƒ½ï¼Œä»¥ä¼˜åŒ–å†…å­˜ä½¿ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†éå¸¸å¤§çš„æ¨¡å‹æ—¶ã€‚
        æ¡ä»¶ï¼šåªæœ‰å½“é…ç½®ï¼ˆself.cfgï¼‰ä¸­çš„ GRADIENT_CHECKPOINTING å±æ€§ä¸º True æ—¶ï¼Œæ‰å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ã€‚è¿™é€šå¸¸åœ¨é…ç½®æ–‡ä»¶ä¸­æŒ‡å®šï¼Œæˆ–åœ¨å®ä¾‹åŒ–æ¨¡å‹ç±»ä¹‹å‰è®¾ç½®ã€‚
        æ–¹æ³•ï¼šgradient_checkpointing_enable() æ˜¯ transformers åº“æä¾›çš„æ–¹æ³•ï¼Œå®ƒå…è®¸æ¨¡å‹åœ¨è®­ç»ƒæ—¶ä»…ä¿å­˜å¿…è¦çš„æ¿€æ´»ï¼Œè€Œåœ¨éœ€è¦æ—¶é‡æ–°è®¡ç®—å…¶ä»–æ¿€æ´»ï¼Œä»è€Œå‡å°‘å†…å­˜æ¶ˆè€—ã€‚
        """
        if self.cfg.GRADIENT_CHECKPOINTING:
            self.model.gradient_checkpointing_enable()

        self.head = nn.Sequential(
            nn.Linear(self.config.hidden_size*4, 32256),
            nn.BatchNorm1d(32256),
            nn.ReLU(),
            nn.Linear(32256, 768),
        )
        """
        self._init_weights æ˜¯ä¸€ä¸ªè‡ªå®šä¹‰æ–¹æ³•ï¼Œç”¨äºåˆå§‹åŒ–ä¼ å…¥æ¨¡å—çš„æƒé‡ã€‚æ­¤æ–¹æ³•é€šå¸¸åŒ…å«é’ˆå¯¹ä¸åŒç±»å‹çš„å±‚ï¼ˆå¦‚çº¿æ€§å±‚ã€åµŒå…¥å±‚ã€å±‚å½’ä¸€åŒ–ç­‰ï¼‰çš„ç‰¹å®šåˆå§‹åŒ–ç­–ç•¥ã€‚
        è¿™ç§åˆå§‹åŒ–åŒ…æ‹¬è®¾ç½®æƒé‡çš„åˆå§‹åˆ†å¸ƒï¼ˆå¦‚æ­£æ€åˆ†å¸ƒï¼‰ï¼Œå¹¶å¯¹åç½®è¿›è¡Œé›¶åˆå§‹åŒ–ç­‰ã€‚
        """
        self._init_weights(self.head)

    # å®šä¹‰ _init_weights çš„æ–¹æ³•ï¼Œç”¨äºè‡ªå®šä¹‰æƒé‡åˆå§‹åŒ–ã€‚
    """
    è¿™ä¸ª _init_weights æ–¹æ³•æä¾›äº†ä¸€ä¸ªæƒé‡åˆå§‹åŒ–ç­–ç•¥ï¼Œ
    é’ˆå¯¹ä¸åŒç±»å‹çš„å±‚é‡‡ç”¨äº†æœ€é€‚åˆçš„åˆå§‹åŒ–æ–¹å¼ã€‚æœ‰åŠ©äºæ¨¡å‹è®­ç»ƒçš„ç¨³å®šæ€§å’Œæ”¶æ•›é€Ÿåº¦ã€‚
    """
    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            """
            çº¿æ€§å±‚ï¼ˆnn.Linearï¼‰ï¼šå¦‚æœ module æ˜¯ä¸€ä¸ªçº¿æ€§å±‚ï¼Œä¸‹é¢çš„æ–¹æ³•å°†ä½¿ç”¨æ­£æ€åˆ†å¸ƒæ¥åˆå§‹åŒ–å…¶æƒé‡ï¼Œå…¶ä¸­å‡å€¼ (mean) ä¸º 0ï¼Œ
            æ ‡å‡†å·® (std) ä¸º self.config.initializer_rangeã€‚è¿™ä¸ª initializer_range é€šå¸¸æ˜¯åœ¨æ¨¡å‹çš„é…ç½®ä¸­æŒ‡å®šçš„ï¼Œç”¨äºæ§åˆ¶åˆå§‹åŒ–çš„åˆ†å¸ƒèŒƒå›´ã€‚
            å¦‚æœè¯¥å±‚åŒ…å«åç½® (bias)ï¼Œåˆ™å°†åç½®åˆå§‹åŒ–ä¸ºé›¶ã€‚
            """
            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)
            if module.bias is not None:
                module.bias.data.zero_()
        elif isinstance(module, nn.Embedding):
            """
            åµŒå…¥å±‚ï¼ˆnn.Embeddingï¼‰ï¼šå¦‚æœ module æ˜¯ä¸€ä¸ªåµŒå…¥å±‚ï¼ŒåŒæ ·ä½¿ç”¨æ­£æ€åˆ†å¸ƒåˆå§‹åŒ–å…¶æƒé‡ã€‚
            å¦‚æœåµŒå…¥å±‚æœ‰ padding_idxï¼ˆä¸€èˆ¬ç”¨äºæ ‡è®°åµŒå…¥çŸ©é˜µä¸­çš„å¡«å……ä½ç½®ï¼‰ï¼Œåˆ™å°†è¿™ä¸ªä½ç½®çš„æƒé‡æ˜¾å¼è®¾ç½®ä¸ºé›¶ã€‚è¿™æ˜¯ä¸ºäº†ç¡®ä¿å¡«å……ä½ç½®çš„åµŒå…¥ä¸ä¼šå¯¹æ¨¡å‹äº§ç”Ÿä»»ä½•å½±å“
            """
            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)
            if module.padding_idx is not None:
                module.weight.data[module.padding_idx].zero_()
        elif isinstance(module, nn.LayerNorm):
            """
            å±‚å½’ä¸€åŒ–ï¼ˆnn.LayerNormï¼‰ï¼šå¦‚æœ module æ˜¯å±‚å½’ä¸€åŒ–å±‚ï¼Œæ–¹æ³•å°†å…¶åç½® (bias) åˆå§‹åŒ–ä¸ºé›¶ï¼Œæƒé‡ (weight) åˆå§‹åŒ–ä¸º 1ã€‚
            å±‚å½’ä¸€åŒ–çš„æƒé‡å’Œåç½®é€šå¸¸ç”¨äºè°ƒæ•´å½’ä¸€åŒ–åæ•°æ®çš„æ¯”ä¾‹å’Œåç§»ã€‚
            """
            module.bias.data.zero_()
            module.weight.data.fill_(1.0)

    def feature(self, inputs):
        """
        self.model(**inputs): è°ƒç”¨é¢„è®­ç»ƒçš„ Transformer æ¨¡å‹ï¼Œä¼ å…¥çš„ inputs å­—å…¸åŒ…å«äº†è¯¸å¦‚ input_ids, attention_mask ç­‰é”®å€¼å¯¹ã€‚
        **inputs æ˜¯ Python çš„è¯­æ³•ï¼Œè¡¨ç¤ºå°†å­—å…¸æ‹†åŒ…ä¸ºå…³é”®å­—å‚æ•°ã€‚
        outputs: æ¨¡å‹çš„è¾“å‡ºé€šå¸¸æ˜¯ä¸€ä¸ªåŒ…å«å¤šä¸ªç»„ä»¶çš„å¯¹è±¡ã€‚å¯¹äºè®¸å¤šåŸºäº Hugging Face çš„ Transformer æ¨¡å‹ï¼Œoutputs åŒ…å«äº† last_hidden_stateï¼ˆæœ€åä¸€å±‚çš„éšè—çŠ¶æ€ï¼‰ï¼Œ
        hidden_statesï¼ˆå¦‚æœé…ç½®äº†è¿”å›æ‰€æœ‰éšè—å±‚çš„çŠ¶æ€ï¼‰ï¼Œä»¥åŠå¯èƒ½çš„ attentionsï¼ˆå¦‚æœé…ç½®äº†è¿”å›æ³¨æ„åŠ›æƒé‡ï¼‰ã€‚
        """
        outputs = self.model(**inputs)
        #last_hidden_states = outputs[1]
        feature1 = self.pool(outputs.hidden_states[-1], inputs['attention_mask'])
        feature2 = self.pool(outputs.hidden_states[-2], inputs['attention_mask'])
        """
        torch.cat([feature1, feature2], dim=1): å°† feature1 å’Œ feature2 æ²¿ç€ç¬¬äºŒç»´åº¦ï¼ˆå³ç‰¹å¾ç»´åº¦ï¼‰æ‹¼æ¥èµ·æ¥ã€‚
        è¿™ç§æ–¹å¼èåˆæ¥è‡ªæœ€åä¸¤ä¸ªéšè—å±‚çš„ä¿¡æ¯ï¼Œä½¿å¾—ç”Ÿæˆçš„ç‰¹å¾å‘é‡ä¸ä»…åŒ…å«äº†æœ€ç»ˆå±‚çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä¹Ÿèå…¥äº†ä¹‹å‰å±‚çš„è¯­ä¹‰ç‰¹å¾ã€‚
        è¿”å›çš„ç»“æœæ˜¯ä¸€ä¸ªæ‰©å±•çš„ç‰¹å¾å‘é‡ï¼Œç°åœ¨çš„ç‰¹å¾å¤§å°æ˜¯åŸæ¥æ¯å±‚ç‰¹å¾å¤§å°çš„ä¸¤å€ï¼Œå› ä¸ºå®ƒåŒ…å«äº†ä¸¤å±‚çš„è¾“å‡ºã€‚
        """
        return torch.cat([feature1, feature2], dim=1)

    def forward(self, original_texts, rewritten_texts, rewrite_prompts_embedding):

        original_texts_feature = self.feature(original_texts) # shape (batch_size, 768)
        rewritten_texts_feature = self.feature(rewritten_texts) # shape (batch_size, 768)
        feature = torch.cat([original_texts_feature, rewritten_texts_feature], dim=1) # shape (batch_size, 768 * 2)
        output = self.head(feature)

        if self.mode == "train":
            prompt_embedding = torch.tensor(rewrite_prompts_embedding, device=self.cfg.DEVICE) # shape (batch_size, 768)
        else:
            prompt_embedding = None

        return output, prompt_embedding
```

#### æ¨¡å‹è®­ç»ƒ
ä¸Šè¿°debertaæ¨¡å‹çš„è®­ç»ƒåœ¨Autodlçš„4090ä¸Š**ä¸€ä¸ªepoch**å¤§çº¦åœ¨**6ä¸ªå°æ—¶å·¦å³**

![image](https://github.com/RoschildRui/RoschildRui.github.io/assets/146306438/c10000e5-7c73-4b27-8d28-fdabf457d8c7)

æˆ‘ä»¬å‘ç°å½“`batch_size=2`ã€ä¸æ‰“å¼€GRADIENT_CHECKPOINTINGæ˜¯æ¯” `batch_size=16`å¹¶æ‰“å¼€GRADIENT_CHECKPOINTINGå¿«ï¼Œå¹¶ä¸”**æµ‹è¯•é›†è¯„ä¼°æ•ˆæœæ²¡æœ‰æ˜¾è‘—çš„å½±å“**ï¼ˆæœ‰äº›æ—¶å€™ææˆäº†ï¼‰ï¼Œäºæ˜¯æˆ‘ä»¬é€‰æ‹©ä¸æ‰“å¼€checkpoint

#### æ¨¡å‹æ¨ç†
å‚è€ƒä»£ç å¦‚ä¸‹ï¼š
```python
def inference_fn(model_weight, config, test_df, tokenizer, device, model_config):
    # ======== DATASETS ==========
    test_dataset = CustomDataset(config, test_df, tokenizer)
    
    # ======== DATALOADERS ==========
    test_loader = DataLoader(
        test_dataset,
        batch_size=config.BATCH_SIZE_TEST,
        shuffle=False,
        num_workers=0,
        pin_memory=True, drop_last=False
    )
    
    # ======== MODEL ==========
    model = CustomModel(config, config_path=model_config, pretrained=False)
    state = torch.load(model_weight)
    model.load_state_dict(state)
    model.to(device)
    model.eval() # set model in evaluation mode
    output_dict = {}
    preds, ids = [], []
    with tqdm(test_loader, unit="test_batch", desc='Test') as tqdm_test_loader:
        for step, batch in enumerate(tqdm_test_loader):
            ids_batch = batch.pop("id")
            original_texts = to_device(collate(batch.pop("original_text")))
            rewritten_texts = to_device(collate(batch.pop("rewritten_text")))
            rewrite_prompts = []
            batch_size = len(ids_batch)
            targets = torch.ones(batch_size, device=device) # -1 for dissimilar, 1 for similar
            with torch.no_grad():
                y_preds, _ = model(original_texts, rewritten_texts, rewrite_prompts)            
            preds.append(y_preds.to('cpu').numpy()) # save predictions
            ids += ids_batch          
    output_dict["predictions"] = np.concatenate(preds) 
    output_dict["ids"] = ids
    return output_dict

preds = []

for model_weight, model_config in zip(model_weights, model_configs):
    predictions = inference_fn(model_weight, config, test_df, tokenizer, device, model_config)
    predictions = predictions["predictions"]
    predictions = torch.nn.functional.normalize(torch.from_numpy(predictions), p=2, dim=1).numpy()
    preds.append(predictions)
    
preds = np.mean(preds, axis=0)

import faiss
from faiss import write_index, read_index, read_VectorTransform

prompts_embedding_index = read_index("./prompts_embedding.index")
search_score, search_index = prompts_embedding_index.search(preds, 1)
prompts_df = pd.read_csv("./prompts_df.csv")
prompts_df.head()

pred_prompts = []

for i, (scr, idx) in tqdm(enumerate(zip(search_score, search_index)), total=len(search_score)):
    scr_idx = idx
    p = prompts_df.loc[scr_idx, "rewrite_prompt"].tolist()
    pred_prompts.append(''.join(p))

values = pred_prompts

submission = pd.DataFrame()
submission["id"] = test_df["id"]
submission["rewrite_prompt"] = values
submission.to_csv("submission_1.csv", index=False)
```

å¥½äº†ç»ˆäºæŠŠç¬¬ä¸€ä¸ªæ¨¡å‹å†™å®Œäº†
![image](https://github.com/RoschildRui/RoschildRui.github.io/assets/146306438/509cd940-f1b4-4e54-a19b-5a010aa0a38e)

### å¾®è°ƒ[phi](https://www.kaggle.com/models/Microsoft/phi/Transformers/2/1)
æ€è·¯æ¥æºäºè¿™ä½å¤§ä½¬å¼€æºçš„[Notebook1](https://www.kaggle.com/code/mozhiwenmzw/0-61-llmpr-phi2-sft-model-generate-infer/notebook)å’Œ[Notebook2](https://www.kaggle.com/code/mozhiwenmzw/0-61-llmpr-phi2-sft-model-training/notebook)

åŒæ—¶æ„Ÿè°¢è¿™ä½å¤§ä½¬å¼€æºçš„[Mean prompt](https://www.kaggle.com/code/seifachour12/lb-score-0-63)

#### è®­ç»ƒadapter
åœ¨çœ‹å®Œå¤§ä½¬çš„ç¬”è®°æœ¬åï¼Œæˆ‘ä»¬å…ˆå°è¯•é€šè¿‡æˆ‘ä»¬è‡ªå·±çš„ç§æœ‰æ•°æ®é›†è®­ç»ƒphiçš„adapterå±‚è¿›è€Œä½¿å¾—å®ƒå¯¹äºè¿™ä¸ªä»»åŠ¡æ›´åŠ é€‚ç”¨

å‚è€ƒä»£ç å¦‚ä¸‹ï¼š
```python
import pandas as pd
from sklearn.model_selection import train_test_split

from datasets import Dataset
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from transformers import TrainingArguments

from trl import SFTTrainer, DataCollatorForCompletionOnlyLM
from peft import LoraConfig

exp_name = 'phi2'
data_path = '/kaggle/input/pr-data/train_clean.csv'
model_path = '/kaggle/input/phi/transformers/2/1'
output_path = f'outputs'
model_save_path =  f'{exp_name}_adapter'

epochs=5
batch_size=1 # 2 
max_seq_length=512 # 1024 
lr = 1e-4

df = pd.read_csv(data_path)
train_df, val_df = train_test_split(df, test_size=0.1, random_state=42)
train_df = train_df.reset_index(drop=True)
val_df = val_df.reset_index(drop=True)

train_ds = Dataset.from_pandas(train_df)
val_ds = Dataset.from_pandas(val_df)

tokenizer = AutoTokenizer.from_pretrained(model_path)
tokenizer.pad_token = tokenizer.eos_token

bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type='nf4',
        bnb_4bit_compute_dtype='float16',
        bnb_4bit_use_double_quant=False,
    )

model = AutoModelForCausalLM.from_pretrained(model_path,
                                             quantization_config=bnb_config,
                                             trust_remote_code=True,
                                             use_auth_token=True)

model.config.gradient_checkpointing = False

def token_len(text):
    tokenized = tokenizer(text, return_length=True)
    length = tokenized['length'][0]
    return length

def formatting_prompts_func(example):
    output_texts = []
    for i in range(len(example['rewritten_text'])):
        ori_text = example['original_text'][i]
        rew_text = example['rewritten_text'][i]
        rew_prompt = example['rewrite_prompt'][i]
        text = f"Instruct: Original Text:{ori_text}\nRewritten Text:{rew_text}\nWrite a prompt that was likely given to the LLM to rewrite original text into rewritten text.Output: {rew_prompt}"
        if token_len(text) > max_seq_length:
            continue
        output_texts.append(text)
    return output_texts

response_template = "Output:"
collator = DataCollatorForCompletionOnlyLM(response_template=response_template, 
                                           tokenizer=tokenizer)

peft_config = LoraConfig(
    r=12,
    lora_alpha=32,
    lora_dropout=0.03,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules= ["q_proj", "k_proj", "v_proj", "dense"],
)

args = TrainingArguments(
    output_dir = output_path,
    fp16=True,
    learning_rate=lr,
    optim="adafactor",
    num_train_epochs=epochs,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size*2,
    gradient_accumulation_steps=8,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    save_total_limit=1,
    logging_steps=50,
    lr_scheduler_type="cosine",
    warmup_ratio=0.1,
    weight_decay=0.008,
    report_to='none',
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    )

trainer = SFTTrainer(
    model=model,
    args = args,
    max_seq_length=max_seq_length,
    train_dataset=train_ds,
    eval_dataset=val_ds,
    formatting_func=formatting_prompts_func,
    data_collator=collator,
    peft_config=peft_config,
)

trainer.train()

trainer.save_model(model_save_path)
tokenizer.save_pretrained(model_save_path)
```
#### åˆ©ç”¨adapterå¾®è°ƒphi
ä½†æ˜¯æˆ‘ä»¬å‘ç°ä¸ç®¡åœ¨phiæ¨¡å‹çš„é¡¶å±‚è¿˜æ˜¯ä¸­é—´å±‚è®­ç»ƒadapterä¼¼ä¹éƒ½æ— æ³•è¾¾åˆ°å¤§ä½¬[å¼€æºç‰ˆæœ¬](https://www.kaggle.com/models/mozhiwenmzw/phi2-public-data-sft-adapter/PyTorch/public-data-sft/1)çš„æ•ˆæœï¼ˆå•æ¨¡æœ€é«˜èƒ½åˆ°`PB=0.63`ä½†æ˜¯é›†æˆå°±ä¼šä½¿å¾—PBç›¸å¯¹ä½¿ç”¨å¼€æºçš„adapterä¸‹é™0.1å·¦å³ï¼‰ğŸ˜…

æ‰€ä»¥æœ€åæˆ‘ä»¬ç›´æ¥ä½¿ç”¨å¼€æºçš„adapterè¿›è¡Œå¾®è°ƒphi

æˆ‘ä»¬å¯¹å¼€æºä»£ç ä¸Šè¿›è¡Œäº†ä¸€äº›è°ƒæ•´ä»¥é’ˆå¯¹æˆ‘ä»¬æœ€åçš„é›†æˆæ–¹æ¡ˆè¿›è¡Œä¼˜åŒ–
- æ ¹æ®å¯¹ç”Ÿæˆæ–‡æœ¬çš„è§‚å¯Ÿï¼Œæ·»åŠ ç¬¦å·'.',';',':',â€œ<|endoftext|>â€ ä½œä¸ºç”Ÿæˆæ–‡æœ¬çš„åœæ­¢æ ‡è®°ï¼Œ**ä¸¥æ ¼æ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„æ—¶é—´**
- å»é™¤ç”Ÿæˆæ–‡æœ¬æœ€åçš„ç¬¦å·ï¼Œæˆ‘ä»¬å‘ç°åœ¨é›†æˆé¢„æµ‹ç»“æœçš„æ—¶å€™è¦ä¸¥æ ¼æ§åˆ¶å¥å·çš„æ•°ç›®ï¼Œå»æ‰å¥å·èƒ½åœ¨PBæé«˜0.01åˆ†å·¦å³ ğŸ¤ 

å‚è€ƒä»£ç å¦‚ä¸‹ï¼š
```python
import numpy as np
import pandas as pd
from tqdm.auto import tqdm
import torch
from peft import PeftConfig, PeftModel
from transformers import AutoModelForCausalLM, AutoTokenizer

input_token_len = 1024
output_token_len = 100
test_df = pd.read_csv('/kaggle/input/llm-prompt-recovery/test.csv')
base_model_name = "/kaggle/input/phi/transformers/2/1"
adapter_model_name = "/kaggle/input/phi2-public-data-sft-adapter/pytorch/public-data-sft/1/phi2_public_data_sft/"
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
tokenizer = AutoTokenizer.from_pretrained(base_model_name,trust_remote_code=True)

if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
    
model = AutoModelForCausalLM.from_pretrained(base_model_name,trust_remote_code=True)
model = PeftModel.from_pretrained(model, adapter_model_name)
model.to(device)
model.eval()

def text_generate(ori_text, rew_text,model, tokenizer, stop_tokens=['.',';',':','<|endoftext|>'], input_max_len=512, output_len=20, device='cuda'):
    prompt = f"Instruct: Original Text:{ori_text}\nRewritten Text:{rew_text}\nWrite a prompt that was likely given to the LLM to rewrite original text to rewritten text.\nOutput:"
    inputs = tokenizer(prompt, max_length=input_max_len, truncation=True, return_tensors="pt", return_attention_mask=False)
    output_start_index = len(inputs.input_ids[0])
    inputs = {k:v.to(device) for k,v in inputs.items()}
    outputs = model.generate(**inputs,
                             do_sample=False,
                             max_new_tokens=output_len,
                             pad_token_id=tokenizer.pad_token_id,
                             eos_token_id=tokenizer.convert_tokens_to_ids(stop_tokens),
                            )
    text = tokenizer.batch_decode(outputs,skip_special_tokens=True,clean_up_tokenization_spaces=False)[0]
    start_index = text.find('Output:')
    generated_text = text[start_index+len('Output:'):].strip()[:-1]
    return generated_text

rewrite_prompts = []
for i, row in tqdm(test_df.iterrows(), total=len(test_df)):
    prompt = mean_prompt = 'Please improve the following text using the writing style of, maintaining the original meaning but altering the tone.'
    try:
        prompt = text_generate(row['original_text'],
                               row['rewritten_text'],
                               model,
                               tokenizer,
                               ['.',';',':','<|endoftext|>'],
                               input_token_len,
                               output_token_len,
                               device,
                              )
    except:
        pass
        
    rewrite_prompts.append(prompt)

test_df['rewrite_prompt'] = rewrite_prompts
sub_df = test_df[['id', 'rewrite_prompt']]
sub_df.to_csv('submission_2.csv', index=False)
```

æœ‰ç‚¹ç¨³äº†æƒ³æ°´ä¸€ä¸‹ğŸ« 

### few-shot mistral-7bæ¨¡å‹
è¿™ä¸ªåº”è¯¥æ˜¯æ¯”èµ›ä¸­æœ€ç«çˆ†çš„æ–¹æ¡ˆï¼Œæ— è®ºå¼€æºè¿˜æ˜¯é—­æº

åŒæ ·ï¼Œè¿™é‡Œæ„Ÿè°¢ä¸€ä¸‹å¤§ä½¬å¼€æºçš„[æ–¹æ¡ˆ](https://www.kaggle.com/code/richolson/mistral-7b-prompt-recovery-version-2)

å‚è€ƒä»£ç å¦‚ä¸‹ï¼š
```python
import torch
import random
import numpy as np
import pandas as pd
import gc
import time

from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

torch.backends.cuda.enable_mem_efficient_sdp(False)
torch.backends.cuda.enable_flash_sdp(False)

if (not torch.cuda.is_available()): print("Sorry - GPU required!")
    
import logging
logging.getLogger('transformers').setLevel(logging.ERROR)
#this can help speed up inference
max_new_tokens = 30

#output test is trimmed according to this
max_sentences_in_response = 1
model_name = '/kaggle/input/mistral-7b-it-v02'
tokenizer = AutoTokenizer.from_pretrained(model_name) 

# Load base model(Mistral 7B)
bnb_config = BitsAndBytesConfig(  
    load_in_4bit= True,
    bnb_4bit_quant_type= "nf4",
    bnb_4bit_compute_dtype= torch.bfloat16,
    bnb_4bit_use_double_quant= False,
)

model = AutoModelForCausalLM.from_pretrained(
        model_name,
        quantization_config=bnb_config,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True,
)
#original text prefix
orig_prefix = "Original Text:"

#mistral "response"
llm_response_for_rewrite = "Provide the new text and I will tell you what new element was added or change in tone was made to improve it - with no references to the original.  I will avoid mentioning names of characters.  It is crucial no person, place or thing from the original text be mentioned.  For example - I will not say things like 'change the puppet show into a book report' - I would just say 'Please improve this text using the writing style of a book report'.  If the original text mentions a specific idea, person, place, or thing - I will not mention it in my answer.  For example if there is a 'dog' or 'office' in the original text - the word 'dog' or 'office' must not be in my response.  My answer will be a single sentence."

#modified text prefix
rewrite_prefix = "Re-written Text:"

#provided as start of Mistral response (anything after this is used as the prompt)
#providing this as the start of the response helps keep things relevant
response_start = "The request was: "

#added after response_start to prime mistral
#"Improve this" or "Improve this text" resulted in non-answers.  
#"Improve this text by" seems to product good results
response_prefix = "Please improve this text using the writing style"

#well-scoring baseline text
#thanks to: https://www.kaggle.com/code/rdxsun/lb-0-61
base_line = 'Please improve this text using the writing style with maintaining the original meaning but altering the tone.' 

#these will all be given to Mistral before each and every prompt
#original_text
#rewritten_text
#prompt

examples_sequences = [
    (
        "Hey there! Just a heads up: our friendly dog may bark a bit, but don't worry, he's all bark and no bite!",
        "Warning: Protective dog on premises. May exhibit aggressive behavior. Ensure personal safety by maintaining distance and avoiding direct contact.",
        "Please improve this text using the writing style of a warning."
    ),

    (
        "A lunar eclipse happens when Earth casts its shadow on the moon during a full moon. The moon appears reddish because Earth's atmosphere scatters sunlight, some of which refracts onto the moon's surface. Total eclipses see the moon entirely in Earth's shadow; partial ones occur when only part of the moon is shadowed.",
        "Yo check it, when the Earth steps in, takes its place, casting shadows on the moon's face. It's a full moon night, the scene's set right, for a lunar eclipse, a celestial sight. The moon turns red, ain't no dread, it's just Earth's atmosphere playing with sunlight's thread, scattering colors, bending light, onto the moon's surface, making the night bright. Total eclipse, the moon's fully in the dark, covered by Earth's shadow, making its mark. But when it's partial, not all is shadowed, just a piece of the moon, slightly furrowed. So that's the rap, the lunar eclipse track, a dance of shadows, with no slack. Earth, moon, and sun, in a cosmic play, creating the spectacle we see today.",
        "Please improve this text using the writing style of a rap."
    ),
    
    (
        "Drinking enough water each day is crucial for many functions in the body, such as regulating temperature, keeping joints lubricated, preventing infections, delivering nutrients to cells, and keeping organs functioning properly. Being well-hydrated also improves sleep quality, cognition, and mood.",
        "Arrr, crew! Sail the health seas with water, the ultimate treasure! It steadies yer body's ship, fights off plagues, and keeps yer mind sharp. Hydrate or walk the plank into the abyss of ill health. Let's hoist our bottles high and drink to the horizon of well-being!",
        "Please improve this text using the writing style of a sea pirate."
    ),
    
    (
        "In a bustling cityscape, under the glow of neon signs, Anna found herself at the crossroads of endless possibilities. The night was young, and the streets hummed with the energy of life. Drawn by the allure of the unknown, she wandered through the maze of alleys and boulevards, each turn revealing a new facet of the city's soul. It was here, amidst the symphony of urban existence, that Anna discovered the magic hidden in plain sight, the stories and dreams that thrived in the shadows of skyscrapers.",
        "On an ordinary evening, amidst the cacophony of a neon-lit city, Anna stumbled upon an anomaly - a door that defied the laws of time and space. With the curiosity of a cat, she stepped through, leaving the familiar behind. Suddenly, she was adrift in the stream of time, witnessing the city's transformation from past to future, its buildings rising and falling like the breaths of a sleeping giant.",
        "Please improve this text using the writing style with time travel topic."
    ),
    
    (
        "Late one night in the research lab, Dr. Evelyn Archer was on the brink of a breakthrough in artificial intelligence. Her fingers danced across the keyboard, inputting the final commands into the system. The lab was silent except for the hum of machinery and the occasional beep of computers. It was in this quiet orchestra of technology that Evelyn felt most at home, on the cusp of unveiling a creation that could change the world.",
        "In the deep silence of the lab, under the watchful gaze of the moon, Dr. Evelyn Archer found herself not alone. Beside her, the iconic red eye of HAL 9000 flickered to life, a silent partner in her nocturnal endeavor. 'Good evening, Dr. Archer,' HAL's voice filled the room, devoid of warmth yet comforting in its familiarity. Together, they were about to initiate a test that would intertwine the destiny of human and artificial intelligence forever. As Evelyn entered the final command, HAL processed the data with unparalleled precision, a testament to the dawn of a new era.",
        "Please improve this text using the writing style with an intelligent computer."
    ),
    
    (
        "The park was empty, save for a solitary figure sitting on a bench, lost in thought. The quiet of the evening was punctuated only by the occasional rustle of leaves, offering a moment of peace in the chaos of city life.",
        "Beneath the cloak of twilight, the park transformed into a realm of solitude and reflection. There, seated upon an ancient bench, was a lone soul, a guardian of secrets, enveloped in the serenity of nature's whispers. The dance of the leaves in the gentle breeze sang a lullaby to the tumult of the urban heart.",
        "Please improve this text using the writing style to be more poetic."
    ),
    
    (
        "The annual town fair was bustling with activity, from the merry-go-round spinning with laughter to the game booths challenging eager participants. Amidst the excitement, a figure in a cloak moved silently, almost invisibly, among the crowd, observing everything with keen interest but participating in none.",
        "Beneath the riot of color and sound that marked the town's annual fair, a solitary figure roamed, known to the few as Eldrin the Enigmatic. Clad in a cloak that shimmered with the whispers of the arcane, Eldrin moved with the grace of a shadow, his gaze piercing the veneer of festivity to the magic beneath. As a master of the mystic arts, he sought not the laughter of the crowds but the silent stories woven into the fabric of the fair. With a flick of his wrist, he could coax wonder from the mundane, transforming the ordinary into spectacles of shimmering illusion, his true participation hidden within the folds of mystery.",
        "Please improve this text using the writing style by adding a magician."
    ),
    
    (
        "The startup team sat in the dimly lit room, surrounded by whiteboards filled with ideas, charts, and plans. They were on the brink of launching a new app designed to make home maintenance effortless for homeowners. The app would connect users with local service providers, using a sophisticated algorithm to match needs with skills and availability. As they debated the features and marketing strategies, the room felt charged with the energy of creation and the anticipation of what was to come.",
        "In the quiet before dawn, a small group of innovators gathered, their mission: to simplify home maintenance through technology. But their true journey began with the unexpected addition of Max, a talking car with a knack for solving problems. 'Let me guide you through this maze of decisions,' Max offered, his dashboard flickering to life.",
        "Please improve this text using the writing style by adding a talking car."
    ),
    
        

    
    
]

def remove_numbered_list(text):
    final_text_paragraphs = [] 
    for line in text.split('\n'):
        # Split each line at the first occurrence of '. '
        parts = line.split('. ', 1)
        # If the line looks like a numbered list item, remove the numbering
        if len(parts) > 1 and parts[0].isdigit():
            final_text_paragraphs.append(parts[1])
        else:
            # If it doesn't look like a numbered list item, include the line as is
            final_text_paragraphs.append(line)

    return '  '.join(final_text_paragraphs)


#trims LLM output to just the response
def trim_to_response(text):
    terminate_string = "[/INST]"
    text = text.replace('</s>', '')
    #just in case it puts things in quotes
    text = text.replace('"', '')
    text = text.replace("'", '')

    last_pos = text.rfind(terminate_string)
    return text[last_pos + len(terminate_string):] if last_pos != -1 else text

#looks for response_start / returns only text that occurs after
def extract_text_after_response_start(full_text):
    parts = full_text.rsplit(response_start, 1)  # Split from the right, ensuring only the last occurrence is considered
    if len(parts) > 1:
        return parts[1].strip()  # Return text after the last occurrence of response_start
    else:
        return full_text  # Return the original text if response_start is not found

    
#trims text to requested number of sentences (or first LF or double-space sequence)
def trim_to_first_x_sentences_or_lf(text, x):
    if x <= 0:
        return ""

    # Any double-spaces dealt with as linefeed
    text = text.replace("  ", "\n")

    # Split text at the first linefeed
    text_chunks = text.split('\n', 1)
    first_chunk = text_chunks[0]

    # Split the first chunk into sentences, considering the space after each period
    sentences = [sentence.strip() for sentence in first_chunk.split('.') if sentence]

    # If there's a linefeed, return the text up to the first linefeed
    if len(text_chunks) > 1:
        # Check if the first chunk has fewer sentences than x, and if so, just return it
        if len(sentences) < x:
            trimmed_text = first_chunk
        else:
            # Otherwise, trim to x sentences within the first chunk
            trimmed_text = '. '.join(sentences[:x]).strip()
    else:
        # If there's no linefeed, determine if the number of sentences is less than or equal to x
        if len(sentences) <= x:
            trimmed_text = '. '.join(sentences).strip()  # Ensure space is preserved after periods
        else:
            # Otherwise, return the first x sentences, again ensuring space after periods
            trimmed_text = '. '.join(sentences[:x]).strip()

    # Add back the final period if it was removed and the text needs to end with a sentence.
    if len(sentences) > 0 and not trimmed_text.endswith('.'):
        trimmed_text += '.'

    return trimmed_text

def get_prompt(orig_text, transformed_text):
    stop_tokens = ['.',':']
    messages = []

    # Append example sequences
    for example_text, example_rewrite, example_prompt in examples_sequences:
        messages.append({"role": "user", "content": f"{orig_prefix} {example_text}"})
        messages.append({"role": "assistant", "content": llm_response_for_rewrite})
        messages.append({"role": "user", "content": f"{rewrite_prefix} {example_rewrite}"})
        messages.append({"role": "assistant", "content": f"{response_start} {example_prompt}"})

    #actual prompt
    messages.append({"role": "user", "content": f"{orig_prefix} {orig_text}"})
    messages.append({"role": "assistant", "content": llm_response_for_rewrite})
    messages.append({"role": "user", "content": f"{rewrite_prefix} {transformed_text}"})
    messages.append({"role": "assistant", "content": f"{response_start}"})
        
    #give it to Mistral
    decode_ids = tokenizer.encode(response_prefix, add_special_tokens=False)
    model_inputs = tokenizer.apply_chat_template(messages, return_tensors="pt")
    
    output_start_index = len(model_inputs[0])
    force_decoder_ids = []
    for i, did in enumerate(decode_ids):
        force_decoder_ids.append([i+output_start_index, did])
    
    model_inputs = model_inputs.to("cuda") 
    generated_ids = model.generate(model_inputs, max_new_tokens=max_new_tokens, 
                                   pad_token_id=tokenizer.eos_token_id,
                                   eos_token_id=tokenizer.convert_tokens_to_ids(stop_tokens),
                                   forced_decoder_ids = force_decoder_ids,
                                  )

    #decode and trim to actual response
    decoded = tokenizer.batch_decode(generated_ids)
    just_response = trim_to_response(decoded[0])        
    final_text = extract_text_after_response_start(just_response)
        
    #mistral has been replying with numbered lists - clean them up....
    final_text = remove_numbered_list(final_text)
        
    #mistral v02 tends to respond with the input after providing the answer - this tries to trim that down
    final_text = trim_to_first_x_sentences_or_lf(final_text, max_sentences_in_response)
    
    #default to baseline if empty or unusually short
    if len(final_text) < 15:
        final_text = base_line
        return final_text
    final_text = final_text[:-1] + ', maintaining the original meaning but altering the tone.'
    return final_text

test_df = pd.read_csv("/kaggle/input/llm-prompt-recovery/test.csv")

for index, row in test_df.iterrows():
    result = get_prompt(row['original_text'], row['rewritten_text'])
    print(result)
    test_df.at[index, 'rewrite_prompt'] = result
    
test_df = test_df[['id', 'rewrite_prompt']]
test_df.to_csv('pred3.csv', index=False)
```


















